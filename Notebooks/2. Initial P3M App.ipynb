{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Imports\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "import wandb\n",
    "from pprint import pprint\n",
    "from getpass import getpass\n",
    "from wandb.integration.openai import autolog\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "from rich.markdown import Markdown\n",
    "import pandas as pd\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential, # for exponential backoff\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:16:33.324683Z",
     "end_time": "2023-08-18T21:16:34.939330Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# load in API key from .env file\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key  = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "if not openai.api_key :\n",
    "    raise ValueError(\"API key not found. Ensure your .env file is correctly set up.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:16:34.939667Z",
     "end_time": "2023-08-18T21:16:34.946644Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# we need a single line of code to start tracing langchain with W&B\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "\n",
    "# wandb documentation to configure wandb using env variables\n",
    "# https://docs.wandb.ai/guides/track/advanced/environment-variables\n",
    "# here we are configuring the wandb project name\n",
    "os.environ[\"WANDB_PROJECT\"] = \"ai_assurance_app\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:16:55.099385Z",
     "end_time": "2023-08-18T21:16:55.112426Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "assert os.getenv(\"WANDB_PROJECT\", \"\") == \"ai_assurance_app\", \"This doesn't look like a valid W&B project\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:16:57.532208Z",
     "end_time": "2023-08-18T21:16:57.539441Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "MODEL_NAME = \"text-davinci-003\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:17:02.091382Z",
     "end_time": "2023-08-18T21:17:02.101835Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # load in pdf document using langchain\n",
    "# import langchain\n",
    "# lc = langchain()\n",
    "# lc.add_pdf(\"Data/guide-to-se-and-p3m-processes.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-10T14:41:59.706799Z",
     "end_time": "2023-08-10T14:41:59.711723Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# load in html data\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:17:06.393572Z",
     "end_time": "2023-08-18T21:17:08.611062Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# loader = UnstructuredHTMLLoader(\"../data/challenges.html\")\n",
    "# loader.load()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T16:11:19.755762Z",
     "end_time": "2023-08-18T16:11:19.765737Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# looks like this strips out the html tags, but needs checking\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "def load_docs_from_directory(directory:str, extension:str=\".html\")->list:\n",
    "    \"\"\"\n",
    "    Loads all documents from a directory with a given extension using langchain\n",
    "    :param directory:str file path to directory containing documents\n",
    "    :param extension:str file extension of documents to load\n",
    "    :return: a list of documents loaded using the loader function\n",
    "    \"\"\"\n",
    "    loader = DirectoryLoader(directory, f\"**/*.{extension}\")\n",
    "    return loader.load()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:17:45.955453Z",
     "end_time": "2023-08-18T21:17:45.960226Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "documents = load_docs_from_directory(\"../data\", \"html\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:17:47.661887Z",
     "end_time": "2023-08-18T21:17:54.904862Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# We will need to count tokens in the documents, and for that we need the tokenizer\n",
    "tokenizer = tiktoken.encoding_for_model(MODEL_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:18:12.242094Z",
     "end_time": "2023-08-18T21:18:12.669513Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[985,\n 1603,\n 1702,\n 789,\n 666,\n 889,\n 486,\n 1379,\n 1320,\n 3926,\n 1989,\n 440,\n 1060,\n 1736,\n 285,\n 553,\n 789,\n 3655,\n 1231,\n 715,\n 1078]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_tokens(documents):\n",
    "    token_counts = [len(tokenizer.encode(document.page_content)) for document in documents]\n",
    "    return token_counts\n",
    "\n",
    "count_tokens(documents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:18:13.128887Z",
     "end_time": "2023-08-18T21:18:13.174233Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "21"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:18:24.781142Z",
     "end_time": "2023-08-18T21:18:24.788095Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## we need to split down the html - determine whether this is possible, or whether converting to text is better (or both)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# text_splitter = CharacterTextSplitter(\n",
    "#     separator = \"\\n\\n\",\n",
    "#     chunk_size = 1000,\n",
    "#     chunk_overlap  = 200,\n",
    "#     length_function = len,\n",
    "#     #is_separator_regex = False,\n",
    "# )\n",
    "#\n",
    "# texts = text_splitter.create_documents(documents)\n",
    "# print(documents[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T16:30:27.602344Z",
     "end_time": "2023-08-18T16:30:27.609302Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# We will use the OpenAIEmbeddings to embed the text, and Chroma to store the vectors\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(documents, embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:18:58.428968Z",
     "end_time": "2023-08-18T21:19:01.014903Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:20:15.735354Z",
     "end_time": "2023-08-18T21:20:15.742704Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import PyPDFLoader\n",
    "# loader = PyPDFLoader(\"../data/guide-to-se-and-p3m-processes.pdf\")\n",
    "# pages = loader.load_and_split()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T13:46:39.359253Z",
     "end_time": "2023-08-18T13:46:41.881975Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We will need to count tokens in the documents, and for that we need the tokenizer\n",
    "tokenizer = tiktoken.encoding_for_model(MODEL_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T13:47:00.431830Z",
     "end_time": "2023-08-18T13:47:02.675598Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-10T14:42:16.765533Z",
     "end_time": "2023-08-10T14:42:16.772265Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function to count the number of tokens in each document\n",
    "def count_tokens(documents):\n",
    "    token_counts = [len(tokenizer.encode(document.page_content)) for document in documents]\n",
    "    return token_counts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T13:47:05.557866Z",
     "end_time": "2023-08-18T13:47:05.564043Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count_tokens(pages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T13:47:06.196822Z",
     "end_time": "2023-08-18T13:47:06.247632Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Notes\n",
    "* what is the maximum number of tokens that can be inputted to the model?\n",
    "* what is going on here (above)? how did we split the pdf into pages? how will this then be passed to the model?\n",
    "* how do you inspect your output when doc splitting? (add to design pattern as an unknown)\n",
    "* interested to know how `pages = loader.load_and_split()` works"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pages[0].page_content\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-09T09:41:33.021048Z",
     "end_time": "2023-08-09T09:41:33.033809Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Markdown(pages[1].page_content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-09T09:42:24.910431Z",
     "end_time": "2023-08-09T09:42:24.929768Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Embeddings\n",
    "* this code use embeddings with a vector database retriever to find relevant documents for a query.\n",
    "* why vector dbs? why not just numpy?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import Chroma\n",
    "#\n",
    "# # We will use the OpenAIEmbeddings to embed the text, and Chroma to store the vectors\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# db = Chroma.from_documents(pages, embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-10T14:42:30.058465Z",
     "end_time": "2023-08-10T14:42:39.865265Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Notes\n",
    "* look up what is going on with retrievers and db stores (add to design pattern as an unknown)\n",
    "* here is a question: what is stored in vector dbs: just the embeddings? or the tokens too?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can create a retriever from the db now, we can pass the `k` param to get the most relevant sections from the similarity search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs=dict(k=3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:21:09.223266Z",
     "end_time": "2023-08-18T21:21:09.229030Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# retriever"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-09T10:29:58.926646Z",
     "end_time": "2023-08-09T10:29:58.935229Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Streaming LangChain activity to W&B at https://wandb.ai/dan-h/ai_assurance_app/runs/cf7j1fzz\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: `WandbTracer` is currently in beta.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the principles of AI Assurance?\"\n",
    "docs = retriever.get_relevant_documents(query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:22:51.709627Z",
     "end_time": "2023-08-18T21:23:00.918264Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/governance.html\n",
      "../data/applying-assurance-techniques.html\n",
      "../data/what-is-assurance.html\n"
     ]
    }
   ],
   "source": [
    "# Let's see the results\n",
    "for doc in docs:\n",
    "    print(doc.metadata[\"source\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:23:08.101008Z",
     "end_time": "2023-08-18T21:23:08.120929Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Notes\n",
    "* look into this: i guess the docs go into the db as embeddings, but retain their link to the source doc? so its interesting to understand how th db works in this regard and what available metadata and normal data is available"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Table of Contents\\n\\nCompliance with regulation\\n\\nManaging risk and building trust\\n\\nAI assurance services are a distinctive and important aspect of broader AI governance. AI governance covers all the means by which the development, use, outputs and impacts of AI can be shaped, influenced and controlled, whether by the government or by those who design, develop, deploy, buy or use these technologies. AI governance includes regulation but also tools like assurance and standards and statements of principles and practice, often referred to as AI ethics.\\n\\nRegulation, standards and other statements of principles and practice define what trustworthy AI looks like. Alongside this, AI assurance services provide the ‘infrastructure’ for checking, assessment and verification against these criteria. Assurance services are needed to evaluate and communicate reliable evidence about the trustworthiness of AI systems against the criteria set out by regulations, standards, principles and norms.\\n\\nAn AI assurance ecosystem can offer an agile regulatory market of assurance services, consisting of both for-profit and not-for-profit services. This regulatory market can support regulators as well as standards development bodies and other responsible AI authorities to achieve compliance goals while enabling industry to innovate at pace and manage risk.\\n\\nAI assurance could play a crucial role in a regulatory environment by providing a toolbox of mechanisms and processes to monitor regulatory compliance as well as the development of common practice beyond statutory requirements to which organisations can be held accountable.\\n\\nCompliance with regulation\\n\\nAI assurance mechanisms facilitatethe implementation of regulation and the monitoring of regulatory compliance in the following ways:\\n\\nImplementation and elaboration of rules for the use of AI systems in specific circumstances.\\n\\nTranslating rules into practical forms useful for end users and evaluating alternative models of implementation.\\n\\nProviding technical expertise and capacity to assess regulatory compliance across the system lifecycle.\\n\\nAssurance mechanisms can also be used to facilitate assessment against designated technical standards that can provide a presumption of conformity with essential legal requirements. For example, the EU’s AI act states that ‘compliance with standards...should be a means for providers to demonstrate conformity with the requirements of this Regulation.’\\n\\nA market of AI assurance services supported by public sector assurance services such as conformity assessment bodies, address critical challenges for ensuring that AI development is safe and beneficial\\n\\nManaging risk and building trust\\n\\nAssurance services also enable stakeholders to ensure compliance with norms and principles of responsible innovation, beyond regulatory compliance. Assurance tools can be effective as post-compliance tools where they can draw on alternative, commonly recognised sources of legitimacy. These might include industry codes of conduct, standards, ethical guidelines, public values, organisational values or preferences/values stated by end users.\\n\\nPost-compliance assurance is particularly useful in the AI context where the complexity of AI systems can make it very challenging to craft meaningful regulation for them. Assurance services offer means to describe, measure, and assign responsibility for AI systems’ impacts, risks and performance without the need to encode explicit, scientific understandings in law.\\n\\nAll content is available under the Open Government License v3.0 except where otherwise stated.\\n\\nPrevWhat is AI assurance and why do we need it?\\n\\nNextNeeds and Responsibilities for AI Assurance?' metadata={'source': '../data/governance.html'}\n",
      "/n-------------------/n\n",
      "page_content=\"For AI, a number of different aspects of an AI system and the broader context in which it is deployed need to be assured. While there are many lists of objectives and risks for AI, there is an emerging consensus about the types of risk and harm, and aspects of performance that need to be understood.\\n\\nPotential subject matter for assuring AI systems:\\n\\nIntended use - is the intended use of the system beneficial and appropriate for the type of system?\\n\\nAccuracy - Is the system accurate and effective in achieving its intended goals?\\n\\nRobustness - Is the system’s performance consistent across a variety of inputs or in different conditions?\\n\\nUnfair bias - Does the system perpetuate unfair bias?\\n\\nExplainability / interpretability - can the reasons behind a system’s decisions or predictions be explained to those affected?\\n\\nSecurity - Is the system vulnerable to cyber attack or malicious use?\\n\\nSocietal impact - Could the operation of the system result in negative consequences for people and society?\\n\\nPrivacy - Does the system ensure individuals privacy rights are respected and protected?\\n\\nHuman rights - Does the system pose a risk to individuals’ human rights?\\n\\nManagement processes and controls - Are the appropriate processes and controls in place to manage risks in the development or deployment of the system?\\n\\nThe levels at which these subject matter could be assessed:\\n\\nInput data\\n\\nTest performance\\n\\nReal world outcomes\\n\\nProcess\\n\\nOrganisations\\n\\nDevelopers\\n\\nConsidering the wide range of potential subject matter for AI Assurance, there is an important challenge to ensure that suitable assurance techniques are adopted, reflecting the characteristics of each subject matter. Deciding on this will involve active participation from across the ecosystem. Government, regulators and standards bodies will need to work closely with industry, academia and civil society to set suitable assurable requirements for AI systems.\\n\\nFor example, the potential societal impacts of an AI system before deployment need to be measured differently to how we would measure the accuracy of an AI system in performing a specific task. In the latter case the accuracy of an AI system can be established with a relatively high level of certainty using a quantitative metric. This is notwithstanding the potential for model drift, which will require ongoing, live testing to ensure the system remains accurate across its lifecycle. In contrast, when thinking about potential societal impacts of an AI system, assessors need to establish potential impacts resulting from a system which would not occur in a counterfactual world in which the technology was not being developed.\\n\\nThis process involves qualitative analysis such as public engagement, futures thinking and consideration of public values. Impact assessments provide assurance to the extent that proper procedures have been followed to identify, mitigate and assign responsibility for impacts - not that all possible impacts have in fact been mitigated (this is an unachievable level of certainty). In contrast, the accuracy of a model against a predetermined standard can be assured to a far greater degree of certainty. While the potential for model drift might decrease this certainty when the system is deployed and used, this can be mitigated via ongoing accuracy testing.\\n\\nWe think that because of these strengths and weaknesses, the assurance techniques listed in section 2.1 provide a useful toolbox of complementary techniques that should be considered when trying to establish trust in AI. However it is important that assurance techniques are coherently and deliberately chosen to suit the nature of the subject matter.\\n\\nTable: Mapping example AI assurance subject matter to suitable assurance techniques\\n\\nAccuracy\\n\\nMeasurable against quantitative metrics. Can be assessed against explicit and objective standards to a relatively high degree of certainty. However, subjective judgement required around what measures of accuracy e.g. precision, sensitivity and benchmarks for acceptable levels of accuracy.\\n\\nPerformance testing, Certification based on performance level, compliance audit for performance testing documentation.\\n\\nThe performance of an AI system e.g. its accuracy in relation to a certain task can be measured against a specific quantitative metric with a relatively high level of certainty and objectivity.To maintain a high-level of certainty and reliability about the accuracy of a system when operating in the wild, the user or an independent assurance provider will need to carry out ongoing monitoring to account for model drift. This is needed to ensure that the model remains accurate as the properties of the input data or target variables change over time.However, there are several different ways of measuring statistical accuracy. For example, you could measure the precision (the percentage of cases identified as positive that are in fact positive)\\u200b\\u200b, or you could measure the sensitivity of an AI system (the percentage of all cases that are positive and that are identified as such).Often, there are trade-offs between these different measurements: improving the precision of the model could lead to the reduction of sensitivity and vice-versa. In these cases, it then becomes a subjective decision about which accuracy metric to prioritise.Due to the tradeoffs between precision and sensitivity, decisions about accuracy will need to be context specific. For example, if using an AI system in a medical imaging device to detect potentially cancerous tumours, sensitivity would be more important. While false positives might risk unwanted distress, false negatives could have deadly consequences.\\n\\nBias\\n\\nBias can be measured quantitatively e.g. measuring an adverse impact ratio. But deciding which measure of bias is appropriate and the level of bias that counts as ‘unfair’ in a specific context are both qualitative judgements.\\n\\nBias audit, Impact assessment is also required to understand the relative severity of potential harms resulting from algorithmic bias.\\n\\nIf assessing an AI system to detect unfair bias, first an appropriate metric needs to be chosen for the use context e.g. demographic parity. Choosing an appropriate metric is, in the first place, subjective. A bias audit can then be carried out by an internal or external team to determine the performance of the algorithm in relation to the chosen metric. This test alone cannot determine whether or not the algorithm is unfairly biased - this requires considering and setting a subjective benchmark for the level of demographic parity that is acceptable. Assurance can be provided to the degree that, according to the chosen benchmark for demographic parity, the algorithm is not unfairly biased.In this case, what is being assured is that an appropriate metric for fairness has been chosen and based upon the results of the audit, the responsible party has determined that the algorithm is not biased according to a known benchmark.\\n\\nSocietal impacts e.g. Human rights Impacts\\n\\nCan be assessed qualitatively with possible quantitative estimates of scope/scale or risk. However societal impacts cannot be observed and are inherently uncertain.\\n\\nImpact assessments\\n\\nImpact assessments are used to identify, manage and mitigate the potential harms to society e.g. discrimination, resulting from or linked to the impacts of an AI system e.g. a disparate error rate.Impacts need to be assessed as proxies for future harms so that responsibility can be appropriately assigned to mitigate these impacts. Impacts cannot be assessed quantitatively due to future uncertainty and the inherent limitations on quantifying harms. Therefore impact assessment must include a qualitative assessment of impacts, including stakeholder engagement and assessing potential impacts in relation to established values. Quantitative analysis of the projected scope and scale of impacts may be used alongside qualitative analysis.Impact assessment cannot assure with certainty that harms will not occur, or that the specified impacts are the only impacts of concern. Assurance can be provided to the degree that known impacts have been identified, accountabilities have been identified for these impacts and mitigation strategies put in place. In this case, what is being assured is that an agreed upon, formalised process for assessing potential societal harms has taken place, and that appropriate accountabilities have been assigned and mitigation strategies put in place.An important but ambiguous question for the assessor is, what impacts are in scope? There are different ways that a line could be drawn around the impacts from deploying an AI system. While direct and immediate harms to rights and freedoms would certainly be in scope, it is unclear whether more distant and indirect harms would need to be considered. For example, job losses resulting from automation over the following decades. Deploying an AI system may contribute to this harm, but might not be a key factor.\\n\\nPrivacy and data protection\\n\\nData protection rating can be measured qualitatively.\\n\\nData Protection Impact Assessment, Privacy Impact assessment, privacy audit for model inferences.\\n\\nWhen assessing the privacy of an AI system throughout its lifecycle, the assessor needs to focus on at least two distinct subject matter areas. Firstly, the data component which can maintain sensitive, personally identifying information, or may be obtained unlawfully. The assessor can carry out a Protection Impact AssessmentData (DPIA), providing a qualitative rating based on the perceived level of data protection.Secondly model inferences need to be assessed as to whether they identify data subjects or groups. This makes measuring and assessing algorithmic privacy partially dependent upon the explainability and interpretability of the AI system. As part of a complex system, the data-algorithm interaction must also be assessed to assure against vulnerabilities arising from the relationship between the data and algorithm. System privacy is also an important subject matter for assuring the security of the system. For example, assessing the system for privacy vulnerabilities leaving it open to adversarial or malicious uses.It is important to note that in some cases, privacy and data protection diverge as subject matter for assurance. For example, model inferences or predictions may impact an individual’s or group’s privacy without involving personal data and engaging data protection. Similarly, data protection regulation contains articles on the right to an explanation for an algorithmic decision, which is not necessarily privacy related.\\n\\nRobustness\\n\\nModel specification can be quantitatively assessed. Qualitative assessment required to establish acceptable benchmarks for robustness for a specific use case.\\n\\nPerformance testing and formal verification required to test the specification of the model - testing the relationship that must hold between the inputs and outputs of the system across variations in input data.\\n\\nThe robustness of an AI system corresponds to how effectively the system can be deemed safe and reliable across variations in the input variables used to make predictions.Evidence about robustness is required to ensure AI systems will be accurate and safe when faced with unexpected events that occur naturally during operation or if they are tampered with by a malicious actor.Different types of AI systems will have different robustness properties and different robustness bounds (acceptable levels of robustness). For example, a computer vision system for an autonomous vehicle will require a high level of robustness due to the serious potential harms of malfunction. Robustness properties for a computer vision system in an autonomous vehicle include: Image noise, deviations from optimal image conditions e.g. a branch partially covering a sign; geometric transformations e.g. road signs shifted at different angles; colour transformations e.g. differences in performance in daylight and nighttime.Performance testing metrics and formal verification can be used to test and quantify a system’s robustness properties. However, judgement is required to decide on the relevant robustness properties and what robustness scores are acceptable.\\n\\nExplainability / interpretability\\n\\nExplainability / interpretability dependent on characteristics of an AI system\\n\\nTechnical AI explainability toolkits, ICO/Turing Explaining Decision made with AI Guidance\\n\\nExplainability refers to the extent to which an AI system provides decisions or suggestions that can be understood by their users and developers. Interpretability refers to how easily cause and effect can be determined for the outputs of a system.\\n\\nAcceptable levels of explainability and interpretability vary depending on whether the use case is high or low risk. In high risk cases, drawing unjustified conclusions from data could cause serious harm.Explainability can be either an inherent characteristic of an AI system or it can be approximated by other methods. This latter type of explainability can be important for so-called ‘black box’ models like artificial neural networks.\\n\\nExplainability and interpretability are not always ends in themselves for assurance but enable systems to be audited and understood to come to conclusions about their trustworthiness. However, explainability and interpretability of a model are important in their own right for developers to check their models beyond just their performance, to rule out that a system is making predictions based upon meta-data rather than the specified input data. A famous example is of an algorithm that distinguished between wolves and huskies, yet this was driven entirely by whether there was a snowy background or not.\\n\\nA further dimension of explainability concerns the right to an explanation. For example, the transparency principle laid out in articles 13-25 in the GDPR requires AI developers and controllers to provide ‘meaningful information about the logic involved’ in a model or algorithm. This problem relates to data protection, but not privacy.\\n\\nThese articles set out a right to an explanation. The ICO/Turing Guidance on Explaining Decisions made with AI sets out requirements for AI developers or controllers for providing meaningful information to explain a decision:\\n\\nThe type of information collected or used in creating the profile or making the automated decision;\\n\\nwhy this information is relevant; and\\n\\nwhat the likely impact is going to be/how it's likely to affect data subjects\\n\\nFor example, if a credit scoring organisation uses an automated process to assess a customer’s creditworthiness. For a traditional credit score, they will need to process data relating to the customer’s credit history, such as payment history, length of credit history, number of credit accounts. An alternative credit scoring agency may additionally use personality data or social media data.\\n\\nThe credit scoring agency will need to explain what type of data is collected to build the credit profile, why this data is relevant to prediction the customers credit score, and the likely impact that such a score will have on the customer I.e. being approved for or denied credit.\\n\\nIntended use / benefit\\n\\nThe scope and acceptable use cases for an AI system need to be qualitatively assessed based on the function of the AI system and the goals for using it. This assessment relies on evidence from quantitative assessment of accuracy, robustness, bias to determine use case suitability.\\n\\nPerformance testing required for assessing robustness and bias audit required for assessing fairness of a system. Impact assessment required for understanding the potential impacts of using an AI system in a specific use case.\\n\\nTo build trust in whether an AI system is suitable for the context or task for which it is being used requires evaluating evidence about it’s performance and robustness and considering the potential societal impacts or safety issues the system might cause if it were to be deployed.For example, the robustness, accuracy, fairness and explainability of an AI system will need to be very high if its intended use is in a medical imaging device. Because of the specific, complex and high-risk nature of medical imaging, an AI system could be certified for a very narrow set of uses or even a single task.In lower risk applications an AI system may be certified for use in a number of different contexts and across a number of different tasks.\\n\\nManagement systems and internal controls\\n\\nManagement systems and internal controls need to be assessed qualitatively but can be assessed objectively against explicit standards and criteria e.g. ISO 9001 Quality Management standards.\\n\\nConformtity assessments, including systems and process audits required to assess conformity of management processes including internal controls and quality management systems with standards, guidelines and regulations.\\n\\nManagement systems and internal controls cover both technical and non technical aspects of AI system development, deployment and use. Management systems are used to assure complex socio-technical processes including quality and risk management.Management system standards provide explicit requirements against which organisational management can be assessed and its trustworthiness understood by assurance users.\\n\\nSecurity\\n\\nAI security can be assured at three levels: 1) Assessment of the product, system or service 2) Assessment of the development process of the product, system or service 3) assessment of the environment, such as the security management and governance.\\n\\nSecurity compliance audits, threat modelling and identification, Red teaming (penetration testing), product/service performance evaluationAvailable examples of technical standards:Applicable to products: ISO/IEC 15408 Information technology—Security techniques—Evaluation criteria for IT securityApplicable to processes: ISO/IEC 21827 :2002 Information technology— Systems Security Engineering—Capability Maturity ModelApplicable to security management: ISO/IEC 27001 Information technology—Security techniques—Information security management systems— Requirements\\n\\nISO/IEC TR 15443 defines these three high-level approaches as follows:For the assessment of a product, system, service - In this case, assurance methods examine the product, system or service and its associated security design documentation independent of the development processes.Assessment of a process involves examining the organisational processes used in the production and operation of the product, system or service throughout its life cycle (i.e., development, deployment, delivery, testing, maintenance, disposal). Assurance is gained through the inference that the processes implemented by people affect the quality of the development and implementation of the product, system or service and, therefore, yield security assurance.Assessment of the environment involves an examination of the environmental factors that contribute to the quality of the processes and the production of the product, system or service. This type of assurance does not examine a deliverable or process directly. These factors assessed include personnel and physical facilities (e.g., development, production, delivery, operation).\\n\\nAll content is available under the Open Government License v3.0 except where otherwise stated.\\n\\nPrevAssurance across the AI system lifecycle\\n\\nNextThe role of independence in assuring AI\" metadata={'source': '../data/applying-assurance-techniques.html'}\n",
      "/n-------------------/n\n",
      "page_content=\"Assurance as a service draws originally from the accounting profession, but has since been adapted to cover many areas such as cyber security and quality management. In these areas, mature ecosystems of assurance products and services enable people to understand whether systems are trustworthy. These products and services include: process and technical standards; repeatable audits; certification schemes; advisory and training services. For example, in financial accounting, auditing services provided by independent accountancy firms enable an assurance user to have confidence in the trustworthiness of the financial information presented by a company.\\n\\nAssurance services can provide the ‘infrastructure’ for checking and verification needed to evaluate and communicate reliable evidence about the trustworthiness of AI systems, against standards, regulations and other principles or guidelines, enabling other actors in the ecosystem to build justified trust in the development and use of these systems.\\n\\nAI assurance services have the potential to play a distinctive and important role within AI governance. It’s not enough to set out standards and rules about how we expect AI systems to be used. It is also important that we have trustworthy information about whether they are following those guidelines.\\n\\nIt is important that meeting these rules and regulations can be checked, both by organisations using these systems and by broader stakeholders affected by their use. Assurance is important both for addressing compliance with rules and regulations, and also for assessing more open ended risks where rules and regulations alone do not provide sufficient guidance to ensure that a system is trustworthy. For example, assessing whether an individual decision made by an AI system is ‘fair’ in a specific context. There will be an important role for consensus based, technical standards here to fill gaps from a non regulatory perspective and to provide guidance on mitigating risks from a technical standpoint.\\n\\nA mature AI assurance ecosystem is needed to coordinate appropriate responsibilities, assurance services, standards and regulations to ensure that those who need to trust AI have the sort of evidence they need to justify that trust.\\n\\nBy ensuring both trust in and the trustworthiness of AI systems, AI assurance will play an important enabling role in the development and deployment of AI, unlocking both the economic and social benefits of AI systems. Consumer trust in AI systems is crucial to widespread adoption, and trustworthiness is essential if systems are going to perform as expected and therefore bring the benefits we want without causing unexpected harm. As we have seen professional services emerge to support other areas from traditional accounting, to cybersecurity services.\\n\\nFor example, the UK's cyber security industry employed 43,000 full-time workers, and contributed nearly £4bn to the UK economy in 2019, according to DCMS. More recently, research commissioned by the Open Data Institute (ODI) on the nascent but buoyant data assurance market found that 890 data assurance firms are now working in the UK with 30,000 staff. The research carried out by Frontier Economics and glass.ai noted that 58% of these firms incorporated in the last 10 years. AI assurance is likely to become a significant economic activity in its own right. AI assurance is an area in which the UK, with particular strengths in legal and professional services, has the potential to excel.\\n\\nAn assurance ecosystem is beginning to emerge for AI, but it is currently immature. AI assurance offers a number of approaches for actors across the AI supply chain to reliably assess, verify and communicate the trustworthiness of AI systems, allowing others in the ecosystem to build justified trust.\\n\\nA challenge for developing effective AI assurance is that AI covers a broad range of complex, emerging technologies being deployed in various contexts. In these different contexts, a number of aspects of an AI system need to be assured to ensure that the whole system is trustworthy. These aspects include the system’s robustness, accuracy, bias and fairness, societal and human rights impacts, data quality, intended use, management processes and controls.\\n\\nTo assure AI systems effectively we will require a toolbox of different products, services and standards suited to assessing these different aspects. For example we might expect to assure the Robustness of AI systems in a more objective and standardisable way than how we assure inherently subjective aspects of automated decision making, such as fairness.\\n\\nAll content is available under the Open Government License v3.0 except where otherwise stated.\\n\\nPrevThe need for trust in AI systems\\n\\nNextWhat is the role of AI assurance in AI governance?\" metadata={'source': '../data/what-is-assurance.html'}\n",
      "/n-------------------/n\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc)\n",
    "    print('/n-------------------/n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:23:25.800177Z",
     "end_time": "2023-08-18T21:23:25.817435Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '../data/governance.html'}\n",
      "{'source': '../data/applying-assurance-techniques.html'}\n",
      "{'source': '../data/what-is-assurance.html'}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:23:22.131575Z",
     "end_time": "2023-08-18T21:23:22.139024Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stuff Prompt\n",
    "\n",
    "We'll now take the content of the retrieved documents, stuff them into prompt template along with the query, and pass into an LLM to obtain the answer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "prompt = PROMPT.format(context=context, question=query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T21:23:50.352491Z",
     "end_time": "2023-08-18T21:23:50.369073Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use langchain to call openai chat API with the question"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Skipping trace saving - unable to safely convert LangChain Run into W&B Trace due to: 'NoneType' object has no attribute 'get'\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens, however you requested 5530 tokens (5274 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidRequestError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OpenAI\n\u001B[1;32m      3\u001B[0m llm \u001B[38;5;241m=\u001B[39m OpenAI()\n\u001B[0;32m----> 4\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m Markdown(response)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/langchain/llms/base.py:826\u001B[0m, in \u001B[0;36mBaseLLM.predict\u001B[0;34m(self, text, stop, **kwargs)\u001B[0m\n\u001B[1;32m    824\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    825\u001B[0m     _stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(stop)\n\u001B[0;32m--> 826\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_stop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/langchain/llms/base.py:786\u001B[0m, in \u001B[0;36mBaseLLM.__call__\u001B[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n\u001B[1;32m    779\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(prompt, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    780\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    781\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    782\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(prompt)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. If you want to run the LLM on multiple prompts, use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    783\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`generate` instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    784\u001B[0m     )\n\u001B[1;32m    785\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 786\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    787\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    788\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    789\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    794\u001B[0m     \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    795\u001B[0m     \u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m    796\u001B[0m )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/langchain/llms/base.py:582\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n\u001B[1;32m    573\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    574\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    575\u001B[0m         )\n\u001B[1;32m    576\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    577\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[1;32m    578\u001B[0m             dumpd(\u001B[38;5;28mself\u001B[39m), [prompt], invocation_params\u001B[38;5;241m=\u001B[39mparams, options\u001B[38;5;241m=\u001B[39moptions\n\u001B[1;32m    579\u001B[0m         )[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    580\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m callback_manager, prompt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(callback_managers, prompts)\n\u001B[1;32m    581\u001B[0m     ]\n\u001B[0;32m--> 582\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    583\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    584\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[1;32m    586\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/langchain/llms/base.py:488\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    486\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n\u001B[1;32m    487\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e)\n\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    489\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m    490\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m manager, flattened_output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(run_managers, flattened_outputs):\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/langchain/llms/base.py:475\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    465\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[1;32m    466\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    467\u001B[0m     prompts: List[\u001B[38;5;28mstr\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    471\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    472\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    473\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    474\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 475\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    476\u001B[0m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    477\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    478\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[1;32m    479\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    480\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    481\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    482\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    483\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[1;32m    484\u001B[0m         )\n\u001B[1;32m    485\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    486\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/langchain/llms/openai.py:399\u001B[0m, in \u001B[0;36mBaseOpenAI._generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    387\u001B[0m     choices\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m    388\u001B[0m         {\n\u001B[1;32m    389\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m: generation\u001B[38;5;241m.\u001B[39mtext,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    396\u001B[0m         }\n\u001B[1;32m    397\u001B[0m     )\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 399\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mcompletion_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    400\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_prompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\n\u001B[1;32m    401\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    402\u001B[0m     choices\u001B[38;5;241m.\u001B[39mextend(response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchoices\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    403\u001B[0m     update_token_usage(_keys, response, token_usage)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/langchain/llms/openai.py:115\u001B[0m, in \u001B[0;36mcompletion_with_retry\u001B[0;34m(llm, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m llm\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 115\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_completion_with_retry\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/tenacity/__init__.py:289\u001B[0m, in \u001B[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(f)\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_f\u001B[39m(\u001B[38;5;241m*\u001B[39margs: t\u001B[38;5;241m.\u001B[39mAny, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: t\u001B[38;5;241m.\u001B[39mAny) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mAny:\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/tenacity/__init__.py:379\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    377\u001B[0m retry_state \u001B[38;5;241m=\u001B[39m RetryCallState(retry_object\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, fn\u001B[38;5;241m=\u001B[39mfn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 379\u001B[0m     do \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/tenacity/__init__.py:314\u001B[0m, in \u001B[0;36mBaseRetrying.iter\u001B[0;34m(self, retry_state)\u001B[0m\n\u001B[1;32m    312\u001B[0m is_explicit_retry \u001B[38;5;241m=\u001B[39m fut\u001B[38;5;241m.\u001B[39mfailed \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fut\u001B[38;5;241m.\u001B[39mexception(), TryAgain)\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_explicit_retry \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry(retry_state)):\n\u001B[0;32m--> 314\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfut\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mafter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mafter(retry_state)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/concurrent/futures/_base.py:437\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    436\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 437\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_condition\u001B[38;5;241m.\u001B[39mwait(timeout)\n\u001B[1;32m    441\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/concurrent/futures/_base.py:389\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 389\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    390\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    391\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    392\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/tenacity/__init__.py:382\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 382\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n\u001B[1;32m    384\u001B[0m         retry_state\u001B[38;5;241m.\u001B[39mset_exception(sys\u001B[38;5;241m.\u001B[39mexc_info())  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/langchain/llms/openai.py:113\u001B[0m, in \u001B[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001B[0;34m(**kwargs)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/openai/api_resources/completion.py:25\u001B[0m, in \u001B[0;36mCompletion.create\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 25\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TryAgain \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     27\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m start \u001B[38;5;241m+\u001B[39m timeout:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m    137\u001B[0m ):\n\u001B[1;32m    138\u001B[0m     (\n\u001B[1;32m    139\u001B[0m         deployment_id,\n\u001B[1;32m    140\u001B[0m         engine,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    150\u001B[0m         api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[1;32m    151\u001B[0m     )\n\u001B[0;32m--> 153\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    164\u001B[0m         \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n\u001B[1;32m    165\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, OpenAIResponse)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/openai/api_requestor.py:298\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    279\u001B[0m     method,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    286\u001B[0m     request_timeout: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    287\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m    288\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_raw(\n\u001B[1;32m    289\u001B[0m         method\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[1;32m    290\u001B[0m         url,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    296\u001B[0m         request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[1;32m    297\u001B[0m     )\n\u001B[0;32m--> 298\u001B[0m     resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/openai/api_requestor.py:700\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response\u001B[0;34m(self, result, stream)\u001B[0m\n\u001B[1;32m    692\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m    693\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interpret_response_line(\n\u001B[1;32m    694\u001B[0m             line, result\u001B[38;5;241m.\u001B[39mstatus_code, result\u001B[38;5;241m.\u001B[39mheaders, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    695\u001B[0m         )\n\u001B[1;32m    696\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m parse_stream(result\u001B[38;5;241m.\u001B[39miter_lines())\n\u001B[1;32m    697\u001B[0m     ), \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 700\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response_line\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    701\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    703\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    704\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    706\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    707\u001B[0m     )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/wandb_edu/lib/python3.8/site-packages/openai/api_requestor.py:763\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response_line\u001B[0;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[1;32m    761\u001B[0m stream_error \u001B[38;5;241m=\u001B[39m stream \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mdata\n\u001B[1;32m    762\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream_error \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m rcode \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m:\n\u001B[0;32m--> 763\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_error_response(\n\u001B[1;32m    764\u001B[0m         rbody, rcode, resp\u001B[38;5;241m.\u001B[39mdata, rheaders, stream_error\u001B[38;5;241m=\u001B[39mstream_error\n\u001B[1;32m    765\u001B[0m     )\n\u001B[1;32m    766\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[0;31mInvalidRequestError\u001B[0m: This model's maximum context length is 4097 tokens, however you requested 5530 tokens (5274 in your prompt; 256 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "response = llm.predict(prompt)\n",
    "Markdown(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-09T11:03:03.021126Z",
     "end_time": "2023-08-09T11:03:04.538774Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Notes\n",
    "* How does the prompt generated here differ from the prompt generated using the chain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = \"What are the principles of P3M?\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-10T14:43:21.766458Z",
     "end_time": "2023-08-10T14:43:21.779488Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n",
    "result = qa.run(query)\n",
    "\n",
    "Markdown(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-10T14:45:48.494767Z",
     "end_time": "2023-08-10T14:45:59.301934Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Notes\n",
    "* need to look at prompt template more thoroughly\n",
    "* understand the `retrieval qa chain` more thoroughly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradio app"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def retrieval_response(message, history):\n",
    "    qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n",
    "    return qa.run(message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-10T14:47:58.353739Z",
     "end_time": "2023-08-10T14:47:58.366528Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_response(message, history):\n",
    "    return random.choice([\"Yes\", \"No\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-10T14:34:27.444462Z",
     "end_time": "2023-08-10T14:34:27.448655Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.ChatInterface(retrieval_response,\n",
    "                 title=\"Ask about P3M\",\n",
    "                chatbot=gr.Chatbot(height=300),\n",
    "                textbox=gr.Textbox(placeholder=\"Ask a question about P3M\", container=False, scale=7),\n",
    "                #description=\"Ask Yes Man any question\",\n",
    "                theme=\"soft\",\n",
    "                examples=[\"What are the key principles of P3M?\", \"What are the limitations of P3M?\"],\n",
    "                cache_examples=True,\n",
    "                retry_btn=None,\n",
    "                undo_btn=\"Delete Previous\",\n",
    "                clear_btn=\"Clear\",\n",
    "            ).launch()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-10T15:05:37.515693Z",
     "end_time": "2023-08-10T15:05:44.227389Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
